http://host3.com/ordinary/6
http://host4.com/ordinary/2
http://host5.com/ordinary/8
http://host6.com/ordinary/7
http://host7.com/ordinary/1
http://host8.com/ordinary/4
http://pdfrobots.host9.com/ordinary/5

# 404 robots.txt, ordinary otherwise -- show allow
http://404.host10.com/ordinary/9

# 500 robots.txt, normal otherwise -- should disallow
http://500.host10.com/ordinary/9

# will redir once on robots and once on the link (to /ordinary/1)
http://302.host10.com/ordinary/0
# and we seed the redir url so it's seen twice and causes a crawled reject
http://302.host10.com/ordinary/1
