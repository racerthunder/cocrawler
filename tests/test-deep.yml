Crawl:
  MaxHostQPS: 10000  # essentially disables rate limiting
  MaxWorkers: 100
  MaxDepth: 100000
  GlobalBudget: 5000 # prevent a runaway
  UserAgent: cocrawler-test/0.01

Plugins:
  url_allowed: SeedsHostname

Fetcher:
  Nameservers:
  - 208.76.50.50
  - 74.82.42.42
  - 198.101.242.72
  - 45.77.165.194
  - 64.6.64.6
  - 81.218.119.11
  - 195.46.39.39
  - 216.146.35.35
  - 91.239.100.100
  - 69.195.152.204
  - 1.1.1.1
  - 8.26.56.26
  - 109.69.8.51
  - 9.9.9.9
  - 199.85.126.10
  - 77.88.8.8
  - 8.8.8.8
  - 208.67.222.222
  - 84.200.69.80
  - 209.244.0.3
  - 37.235.1.174
  - 156.154.70.1
  NameserverTries: 10
  NameserverTimeout: 3.0
  CrawlLocalhost: False  # crawl ips that resolve to localhost
  CrawlPrivate: False  # crawl ips that resolve to private networks (e.g. 10.*/8)
  DNSCacheMaxSize: 1000000

Multiprocess:
  ParseInBurnerSize: 1 # make sure the burner thread gets used 100%
  Affinity: yes

Seeds:
  Hosts:
  - http://test.website/ordinary/0
  - http://test.website/ordinary/1 # makes the robots fetch interlock fire
  CrawledHosts:
  - http://test.website/ordinary/3

Logging:
  Crawllog: crawllog.jsonl
  Frontierlog: frontierlog
  Robotslog: robotslog.jsonl

UserAgent:
  Style: crawler
  MyPrefix: test-deep
  URL: http://example.com/cocrawler.html

Testing:
  TestHostmapAll: 127.0.0.1:8080
#  TestHostmapAll: localhost:8080 ## aiodns doesn't consult /etc/hosts :-/
  StatsEQ:
    fetch URLs: 999
    fetch http code=200: 999
    max urls found on a page: 3
    robots denied: 999  # the url ^/denied/ is on every page
    warc r/r (prefix Testing): 999
    parser in burner thread: 999
    parser in main thread: 0

System:
  RLIMIT_AS_gigabytes: 8
